Index: modules/streaming_pipeline/streaming_pipeline/qdrant.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nfrom typing import Optional\n\nfrom bytewax.outputs import DynamicOutput, StatelessSink\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.api_client import UnexpectedResponse\nfrom qdrant_client.http.models import Distance, VectorParams, OptimizersConfigDiff\nfrom qdrant_client.models import PointStruct\n\nfrom streaming_pipeline import constants\nfrom streaming_pipeline.models import Document\n\nimport os\nfrom typing import List, Dict, Optional\nfrom qdrant_client.models import PointStruct\nfrom qdrant_client.http.models import VectorParams, Distance\nfrom qdrant_client import QdrantClient\nimport openai\n\nclass HierarchicalDataManager:\n\n    def __init__(self, qdrant_client: QdrantClient):\n        self.client = qdrant_client\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n        self.hierarchy_collection = \"hierarchy_tree\"\n\n        # Ensure the hierarchy collection exists\n        try:\n            self.client.get_collection(collection_name=self.hierarchy_collection)\n        except Exception:\n            # Create the hierarchy collection if it doesn't exist\n            self.client.create_collection(\n                collection_name=self.hierarchy_collection,\n                vectors_config=VectorParams(size=1, distance=Distance.COSINE),  # Dummy vector size\n            )\n\n\n    def classify_with_gpt(self, text: str, options: List[str], level: str) -> str:\n        \"\"\"\n        Use GPT to classify text into one of the given options.\n        \"\"\"\n        prompt = (\n            f\"Based on the following text, decide which {level} it belongs to:\\n\\n\"\n            f\"Text: {text}\\n\\n\"\n            f\"Options: {', '.join(options)}\\n\\n\"\n            f\"Only return the name of the {level}.\"\n        )\n        response = openai.Completion.create(\n            engine=\"text-davinci-003\",\n            prompt=prompt,\n            max_tokens=20,\n            temperature=0.0\n        )\n        return response.choices[0].text.strip()\n\n    def get_hierarchy_node(self, name: str, level: str) -> Optional[Dict]:\n        \"\"\"\n        Retrieve a hierarchy node by its name and level.\n        \"\"\"\n        results = self.client.search(\n            collection_name=self.hierarchy_collection,\n            query_vector=[1.0],  # Dummy query vector\n            filter={\n                \"must\": [\n                    {\"key\": \"name\", \"match\": {\"value\": name}},\n                    {\"key\": \"type\", \"match\": {\"value\": level}},\n                ]\n            },\n            limit=1,\n        )\n        return results[0].payload if results else None\n\n    def save_hierarchy_node(self, name: str, level: str, parent: Optional[str] = None, children: Optional[List[str]] = None):\n        \"\"\"\n        Save or update a hierarchy node.\n        \"\"\"\n        node = self.get_hierarchy_node(name, level)\n        if node:\n            # Update the existing node\n            node[\"children\"] = list(set(node.get(\"children\", []) + (children or [])))\n            self.client.upsert(\n                collection_name=self.hierarchy_collection,\n                points=[\n                    PointStruct(\n                        id=node[\"id\"],\n                        vector=[0.0],  # Dummy vector\n                        payload=node,\n                    )\n                ],\n            )\n        else:\n            # Create a new node\n            self.client.upsert(\n                collection_name=self.hierarchy_collection,\n                points=[\n                    PointStruct(\n                        id=None,\n                        vector=[0.0],  # Dummy vector\n                        payload={\n                            \"type\": level,\n                            \"name\": name,\n                            \"parent\": parent,\n                            \"children\": children or [],\n                        },\n                    )\n                ],\n            )\n\n    def save_data(self, document):\n        \"\"\"\n        Save a document in the proper hierarchical structure.\n        \"\"\"\n        document_text = ' '.join(document.text)\n\n        # Step 1: Sector Classification\n        sectors = [\n            node[\"name\"] for node in self.client.search(\n                collection_name=self.hierarchy_collection,\n                query_vector=[1.0],\n                filter={\"must\": [{\"key\": \"type\", \"match\": {\"value\": \"sector\"}}]},\n                limit=100,\n            )\n        ]\n        sector = self.classify_with_gpt(document_text, sectors, \"sector\")\n        self.save_hierarchy_node(name=sector, level=\"sector\")\n\n        # Step 2: Company/Subject Classification\n        subjects = [\n            node[\"name\"] for node in self.client.search(\n                collection_name=self.hierarchy_collection,\n                query_vector=[1.0],\n                filter={\n                    \"must\": [\n                        {\"key\": \"type\", \"match\": {\"value\": \"subject\"}},\n                        {\"key\": \"parent\", \"match\": {\"value\": sector}},\n                    ]\n                },\n                limit=100,\n            )\n        ]\n        subject = self.classify_with_gpt(document_text, subjects, \"subject\")\n        self.save_hierarchy_node(name=subject, level=\"subject\", parent=sector)\n\n        # Step 3: Event Type Classification\n        event_types = [\n            node[\"name\"] for node in self.client.search(\n                collection_name=self.hierarchy_collection,\n                query_vector=[1.0],\n                filter={\n                    \"must\": [\n                        {\"key\": \"type\", \"match\": {\"value\": \"event_type\"}},\n                        {\"key\": \"parent\", \"match\": {\"value\": subject}},\n                    ]\n                },\n                limit=100,\n            )\n        ]\n        event_type = self.classify_with_gpt(document_text, event_types, \"event type\")\n        self.save_hierarchy_node(name=event_type, level=\"event_type\", parent=subject)\n\n        # Step 4: Save the document in its specific Qdrant collection\n        collection_name = f\"{sector}_{subject}_{event_type}\".lower().replace(\" \", \"_\")\n        if not self.client.get_collection(collection_name):\n            self.client.create_collection(\n                collection_name,\n                vectors_config=VectorParams(size=len(vector), distance=Distance.COSINE),\n            )\n        ids, payloads = document.to_payloads()\n        points = [\n            PointStruct(id=idx, vector=vector, payload=_payload)\n            for idx, vector, _payload in zip(ids, document.embeddings, payloads)\n        ]\n\n        self.client.upsert(collection_name=collection_name, points=points)\n\nclass QdrantVectorOutput(DynamicOutput):\n    \"\"\"A class representing a Qdrant vector output.\n\n    This class is used to create a Qdrant vector output, which is a type of dynamic output that supports\n    at-least-once processing. Messages from the resume epoch will be duplicated right after resume.\n\n    Args:\n        vector_size (int): The size of the vector.\n        collection_name (str, optional): The name of the collection.\n            Defaults to constants.VECTOR_DB_OUTPUT_COLLECTION_NAME.\n        client (Optional[QdrantClient], optional): The Qdrant client. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int,\n        collection_name: str = constants.VECTOR_DB_OUTPUT_COLLECTION_NAME,\n        client: Optional[QdrantClient] = None,\n    ):\n        self._collection_name = collection_name\n        self._vector_size = vector_size\n\n        if client:\n            self.client = client\n        else:\n            self.client = build_qdrant_client()\n\n        try:\n            self.client.get_collection(collection_name=self._collection_name)\n        except (UnexpectedResponse, ValueError):\n            self.client.recreate_collection(\n                collection_name=self._collection_name,\n                vectors_config=VectorParams(\n                    size=self._vector_size, distance=Distance.COSINE\n                ),\n                # Manuall add this optimizers_config to address issue: https://github.com/iusztinpaul/hands-on-llms/issues/72\n                # qdrant_client.http.exceptions.ResponseHandlingException: 1 validation error for ParsingModel[InlineResponse2005] (for parse_as_type)\n                # obj -> result -> config -> optimizer_config -> max_optimization_threads\n                # none is not an allowed value (type=type_error.none.not_allowed)\n                optimizers_config=OptimizersConfigDiff(max_optimization_threads=1),\n            )\n\n    def build(self, worker_index, worker_count):\n        \"\"\"Builds a QdrantVectorSink object.\n\n        Args:\n            worker_index (int): The index of the worker.\n            worker_count (int): The total number of workers.\n\n        Returns:\n            QdrantVectorSink: A QdrantVectorSink object.\n        \"\"\"\n\n        return QdrantVectorSink(self.client, self._collection_name)\n\n\ndef build_qdrant_client(url: Optional[str] = None, api_key: Optional[str] = None):\n    \"\"\"\n    Builds a QdrantClient object with the given URL and API key.\n\n    Args:\n        url (Optional[str]): The URL of the Qdrant server. If not provided,\n            it will be read from the QDRANT_URL environment variable.\n        api_key (Optional[str]): The API key to use for authentication. If not provided,\n            it will be read from the QDRANT_API_KEY environment variable.\n\n    Raises:\n        KeyError: If the QDRANT_URL or QDRANT_API_KEY environment variables are not set\n            and no values are provided as arguments.\n\n    Returns:\n        QdrantClient: A QdrantClient object connected to the specified Qdrant server.\n    \"\"\"\n\n    if url is None:\n        try:\n            url = os.environ[\"QDRANT_URL\"]\n        except KeyError:\n            raise KeyError(\n                \"QDRANT_URL must be set as environment variable or manually passed as an argument.\"\n            )\n\n    if api_key is None:\n        try:\n            api_key = os.environ[\"QDRANT_API_KEY\"]\n        except KeyError:\n            raise KeyError(\n                \"QDRANT_API_KEY must be set as environment variable or manually passed as an argument.\"\n            )\n\n    client = QdrantClient(url, api_key=api_key)\n\n    return client\n\n\nclass QdrantVectorSink(StatelessSink):\n    \"\"\"\n    A sink that writes document embeddings to a Qdrant collection.\n\n    Args:\n        client (QdrantClient): The Qdrant client to use for writing.\n        collection_name (str, optional): The name of the collection to write to.\n            Defaults to constants.VECTOR_DB_OUTPUT_COLLECTION_NAME.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: QdrantClient,\n        collection_name: str = constants.VECTOR_DB_OUTPUT_COLLECTION_NAME,\n    ):\n        self._collection_name = collection_name\n        self._openai_client=HierarchicalDataManager(client)\n\n    def write(self, document: Document):\n        self._openai_client.save_data(document)\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/modules/streaming_pipeline/streaming_pipeline/qdrant.py b/modules/streaming_pipeline/streaming_pipeline/qdrant.py
--- a/modules/streaming_pipeline/streaming_pipeline/qdrant.py	(revision abfe80e7f8f88a6c7152727493ef80d051e1de90)
+++ b/modules/streaming_pipeline/streaming_pipeline/qdrant.py	(date 1736517506554)
@@ -40,7 +40,7 @@
         Use GPT to classify text into one of the given options.
         """
         prompt = (
-            f"Based on the following text, decide which {level} it belongs to:\n\n"
+            f"Based on the following text, decide which {level} it belongs to, and if not exist please create new one:\n\n"
             f"Text: {text}\n\n"
             f"Options: {', '.join(options)}\n\n"
             f"Only return the name of the {level}."
@@ -97,9 +97,9 @@
                         id=None,
                         vector=[0.0],  # Dummy vector
                         payload={
-                            "type": level,
-                            "name": name,
-                            "parent": parent,
+                            "type": level, #sector/subject/eventtype
+                            "name": name, #finance, technology . medincn
+                            "parent": parent, #
                             "children": children or [],
                         },
                     )
@@ -111,7 +111,7 @@
         Save a document in the proper hierarchical structure.
         """
         document_text = ' '.join(document.text)
-
+        #sector -> #subject(company, currency) ->#event type
         # Step 1: Sector Classification
         sectors = [
             node["name"] for node in self.client.search(
